---
title: "Chapter 09. Hierarchical Models"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  github_document
---

# Hierarchical Models

As Kruschke put it: "There are many realistic situations that involve meaningful hierarchical structure. Bayesian modeling software makes it straightforward to specify and analyze complex hierarchical models" (p. 221). IMO, brms makes it even easier than JAGS.

## A single coin from a single mint

Recall from the last chapter that our likelihood is the Bernoulli distribution:

$$y_{i} \sim Bernoulli(\theta)$$

We'll use the Beta density for our prior distribution for $\theta$:

$$\theta \sim Beta(\alpha, \beta)$$

And we can re-express $\alpha$ and $\beta$ in terms of the mode $\omega$ and concentration $\kappa$, such that

$$\alpha = \omega(\kappa - 2) + 1 \textrm{ and } \beta = (1 - \omega)(\kappa - 2) + 1$$

The consequence of this is that we can re-express $\theta$ as

$$\theta \sim Beta(\omega(\kappa - 2) + 1, (1 - \omega)(\kappa - 2) + 1)$$

On page 224, Kruschke wrote: "The value of $\kappa$ governs how near $\theta$ is to $\omega$, with larger values of $\kappa$ generating values of $\theta$ more concentrated near $\omega$." To give a sense of that, we'll simulate 20 beta distributions, all with $\omega = .25$ but with $\theta$ increasing from 10 to 200, by 10.

```{r, fig.width = 5, fig.height = 5, message = F, warning = F}
library(tidyverse)
library(ggridges)

beta_by_k <- function(k){
  w <- .25
  tibble(x = seq(from = 0, to = 1, length.out = 1000)) %>% 
    mutate(theta = dbeta(x = x,
                         shape1 = w * (k - 2) + 1,
                         shape2 = (1 - w) * (k - 2) + 1))
}

tibble(k = seq(from = 10, to = 200, by = 10)) %>% 
  mutate(theta = map(k, beta_by_k)) %>% 
  unnest() %>%
  
  ggplot(aes(x = x, y = k,
             height = theta,
             group = k, fill = k)) +
  geom_vline(xintercept = c(0, .25, .5), color = "grey85", size = 1/2) +
  geom_ridgeline(size = 1/4, color = "white", scale = 2) +
  scale_fill_viridis_c(expression(kappa), option = "A") +
  scale_y_continuous(breaks = seq(from = 10, to = 200, by = 10)) +
  labs(x = expression(theta),
       y = expression(kappa)) +
  theme(panel.grid = element_blank())
```

Holding $\omega$ constant, the density gets more concentrated around $\omega$ as $\kappa$ increases.

### Posterior via grid approximation.

Given $\alpha$ and $\beta$, we can compute their mode $\omega$. To foreshadow, consider $\text{Beta} (2, 2)$.

```{r}
alpha <- 2
beta  <- 2

(alpha - 1) / (alpha + beta - 2)
```

That is, the mode of $\text{Beta} (2, 2) = .5$. 

We won’t be able to make the wireframe plots on the left of Figure 9.2, but we can make the others. We'll make the initial data following Kruschke's (p. 226) formulas

$$p(\theta, \omega) = p(\theta | \omega) p(\omega) = \text{Beta} (\theta | \omega (100 - 2) + 1, (1 - \omega) (100 - 2) + 1) \text{Beta} (\omega | 2, 2)$$

First, we'll make a custom function, `make_prior()` based on the formulas.

```{r}
make_prior <- function(theta, omega){
  dbeta(x = theta,
        shape1 =      omega  * (kappa - 2) + 1,
        shape2 = (1 - omega) * (kappa - 2) + 1) *
    dbeta(x = omega,
          shape1 = alpha,
          shape2 = beta)
  }
```

We already assigned values to `alpha` and `beta`, above. They're both 2. Next we'll define the parameter space as a sequence from 0 to 1 and assign the value of 100 to `kappa`.

```{r}
parameter_space <- seq(from = 0, to = 1, by = .01)
kappa <- 100
```

Our `make_prior()` function had five variables. We've saved the values of three of them into working memory (i.e., `alpha`, `beta`, and `kappa`). Now we'll use `parameter_space` to define the ranges for the final two variables, `theta` and `omega`, which we'll save in a tibble. We'll then feed the `theta` and `omega` values into our function via `purrr::map2()`, which will return a nested tibble. Once we `unnest()` the tibble, we'll be ready to plot the top middle panel of Figure 9.2.

```{r, fig.width = 3.5, fig.height = 3}
d <-
  tibble(theta = parameter_space,
         omega = parameter_space) %>%
  expand(theta, omega) %>% 
  mutate(prior = map2(theta, omega, make_prior)) %>% 
  unnest() %>% 
  # here we normalize (i.e., translate the prior from the density metric to the probability metric)
  mutate(prior = prior / sum(prior))

d %>% 
  ggplot(aes(x = theta, y = omega, fill = prior)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

If we collapse "the joint prior across $\theta$" (i.e., `group_by(omega)` and then `sum(prior)`), we plot the marginal distribution for $p(\omega)$ as seen in the top right panel.

```{r, fig.width = 3.5, fig.height = 3}
d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("Marginal p(", omega, ")"))) +
  coord_flip(ylim = c(0, .03)) +
  theme(panel.grid = element_blank())
```

We'll follow a similar procedure to get the marginal probability distribution for `theta`.

```{r, fig.width = 3.5, fig.height = 3}
d %>%
  group_by(theta) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, ")"))) +
  coord_cartesian(ylim = c(0, .03)) +
  theme(panel.grid = element_blank())
```

With a little `filter()`ing and wrangling, we’ll be in good shape to make the two short plots in the right panel of the second row from the top.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(label = str_c("omega == ", omega)) %>% 
  mutate(label = factor(label, levels = c("omega == 0.75", "omega == 0.25"))) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta, "|", omega, ")"))) +
  coord_cartesian() +
  theme(panel.grid = element_blank()) +
  facet_wrap(~label, ncol = 1, labeller = label_parsed)
```

As Kruschke pointed out at the top of page 228, these are indeed Beta densities.

```{r, fig.width = 3.5, fig.height = 1.75}
# we'll want this for the annotation
text <-
  tibble(theta = c(.75, .25),
         y = 10.5,
         label = c("Beta(74.5, 25.5)", "Beta(25.5, 74.5)"),
         omega = letters[1:2])

# here's the primary data for the plot
tibble(theta = rep(parameter_space, times = 2),
       alpha = rep(c(74.5, 25.5),   each = 101),
       beta  = rep(c(25.5, 74.5),   each = 101),
       omega = rep(letters[1:2],    each = 101)) %>%
  
  # the plot
  ggplot(aes(x     = theta,
             ymin  = 0,
             ymax  = dbeta(x = theta, shape1 = alpha, shape2 = beta), 
             fill  = omega)) +
  geom_ribbon() +
  geom_text(data = text,
            aes(y = y, label = label, color = omega)) +
  scale_fill_viridis_d(option = "A", begin = .3, end = .7) +
  scale_color_viridis_d(option = "A", begin = .3, end = .7) +
  labs(x = expression(theta),
       y = "density") +
  coord_cartesian(ylim = 0:12) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

We need the Bernoulli likelihood function for the lower three rows of Figure 9.2.

```{r}
Bernoulli_likelihood <- function(theta, data) {
  # theta = success probability parameter ranging from 0 to 1
  # data = the vector of data (i.e., a series of 0s and 1s)
  N <- length(data)
  z <- sum(data)
  return(theta^z * (1 - theta)^(N - sum(data)))
  }
```

Time to feed `theta` and our data into `Bernoulli_likelihood()`, which will allow us to make the 2-dimensional density plot in the middle of Figure 9.2.

```{r, fig.width = 3.5, fig.height = 3}
N <- 12
z <- 9

trial_data <- rep(0:1, times = c(N - z, z))

d <-
  d %>% 
  mutate(likelihood = Bernoulli_likelihood(theta = theta, 
                                           data  = trial_data))

d %>%
  ggplot(aes(x = theta, y = omega, fill = likelihood)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

From formula 9.1, the posterior $p(\theta, \omega| D)$ is proportional to $p(D|\theta) p(\theta|\omega) p(\omega)$. Divide by the normalizing constant and we'll have it in a proper probability metric.

Our first depiction will be the middle panel of the second row from the bottom.

```{r, fig.width = 3.5, fig.height = 3}
d <-
  d %>% 
  mutate(posterior = (likelihood * prior * omega) / sum(likelihood * prior * omega)) 

d %>% 
  ggplot(aes(x = theta, y = omega, fill = posterior)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

Making the marginal plots for `posterior` is much like when making them for `prior`, above.

```{r, fig.width = 3.5, fig.height = 3}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("Marginal p(", omega, "|D)"))) +
  coord_flip() +
  theme(panel.grid = element_blank())

# for theta
d %>%
  group_by(theta) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, "|D)"))) +
  coord_cartesian() +
  theme(panel.grid = element_blank())
```

And like above, we'll `filter()` and wrangle to make the short plots in the lower right panel of Figure 9.2.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(label = str_c("omega == ", omega)) %>% 
  mutate(label = factor(label, levels = c("omega == 0.75", "omega == 0.25"))) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta, "|", omega, ")"))) +
  # coord_cartesian(ylim = 0:10) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~label, ncol = 1, labeller = label_parsed, scales = "free")
```

Before we repeat the process for Figure 9.3, we'll redefine our constants.

```{r}
alpha <- 20
beta  <- 20
kappa <- 6
```

Here's the initial data and the 2-dimensional density plot for the prior.

```{r, fig.width = 3.5, fig.height = 3}
d <-
  tibble(theta = parameter_space,
         omega = parameter_space) %>%
  expand(theta, omega) %>% 
  mutate(prior = map2(theta, omega, make_prior)) %>% 
  unnest() %>% 
  mutate(prior = prior / sum(prior))

d %>% 
  ggplot(aes(x = theta, y = omega, fill = prior)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

Now we'll average over `omega` and `theta` to plot their marginal prior distributions.

```{r, fig.width = 3.5, fig.height = 3}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("Marginal p(", omega, ")"))) +
  coord_flip() +
  theme(panel.grid = element_blank())

# for theta
d %>%
  group_by(theta) %>% 
  summarise(prior = sum(prior)) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, ")"))) +
  coord_cartesian(ylim = c(0, .03)) +
  theme(panel.grid = element_blank())
```

Here are the two short plots in the right panel of the second row from the top of Figure 9.3.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(label = str_c("omega == ", omega)) %>% 
  mutate(label = factor(label, levels = c("omega == 0.75", "omega == 0.25"))) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta, "|", omega, ")"))) +
  coord_cartesian(ylim = c(0, 6e-06)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~label, ncol = 1, labeller = label_parsed)
```

Now we're ready for the likelihood.

```{r, fig.width = 3.5, fig.height = 3}
d <-
  d %>% 
  mutate(likelihood = Bernoulli_likelihood(theta = theta, 
                                           data  = trial_data))

d %>%
  ggplot(aes(x = theta, y = omega, fill = likelihood)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

Here's the posterior, $p(\theta|\text{D})$. Our first depiction will be the middle panel of the second row from the bottom of Figure 9.3.

```{r, fig.width = 3.5, fig.height = 3}
d <-
  d %>% 
  # mutate(marginal_likelihood = sum(prior * likelihood)) %>% 
  mutate(posterior = (likelihood * prior * omega) / sum(likelihood * prior * omega)) 

d %>% 
  ggplot(aes(x = theta, y = omega, fill = posterior)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

Here are the marginal plots for `posterior`.

```{r, fig.width = 3.5, fig.height = 3}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("Marginal p(", omega, "|D)"))) +
  coord_flip() +
  theme(panel.grid = element_blank())

# for theta
d %>%
  group_by(theta) %>% 
  summarise(posterior = sum(posterior)) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, "|D)"))) +
  coord_cartesian() +
  theme(panel.grid = element_blank())
```

And we'll finish off with the plots of Figure 9.3's lower right panel.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(label = str_c("omega == ", omega)) %>% 
  mutate(label = factor(label, levels = c("omega == 0.75", "omega == 0.25"))) %>% 
  
  ggplot(aes(x = theta,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta, "|", omega, ")"))) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~label, ncol = 1, labeller = label_parsed, scales = "free")
```

## Multiple coins from a single mint

Kruschke provided our motivation on page 230: "What if we collect data from more than one coin created by the mint? If each coin has its own distinct bias $\theta$s, then we are estimating a distinct parameter value for each coin, and using all the data to estimate $\omega$."

### Posterior via grid approximation.

Here's my attempt at the multiple coins prior.

```{r}
alpha <- 2
beta  <- 2
kappa <- 5

d <-
  tibble(theta_1 = parameter_space,
         omega   = parameter_space) %>%
  expand(theta_1, omega) %>% 
  # the second column of theta values is a duplicate of the first
  mutate(theta_2 = theta_1) %>% 
  mutate(prior_1 = map2(theta_1, omega, make_prior),
         prior_2 = map2(theta_2, omega, make_prior)) %>% 
  unnest() %>% 
  # here we normalize
  mutate(prior_1 = prior_1 / sum(prior_1),
         prior_2 = prior_2 / sum(prior_2))

head(d)  
```

The left and middle columns of the top two rows are the same save for their indices. So I'm just going to focus on those from `theta_1`. Here's the 2-dimensional density of the upper left panel.

```{r, fig.width = 3.5, fig.height = 3}
d %>% 
  ggplot(aes(x = theta_1, y = omega, fill = prior_1)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[1]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

Now we'll average over `omega` and `theta` to plot their marginal prior distributions.

```{r, fig.width = 3.5, fig.height = 3}
# for omega
d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior_1 + prior_2)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("p(", omega, ")"))) +
  coord_flip(ylim = c(0, .08)) +
  theme(panel.grid = element_blank())

# for theta
d %>%
  group_by(theta_1) %>% 
  summarise(prior = sum(prior_1 + prior_2)) %>% 

  ggplot(aes(x = theta_1,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta[1]),
       y = expression(paste("p(", theta[1], ")"))) +
  coord_cartesian(ylim = c(0, .08)) +
  theme(panel.grid = element_blank())
```

Now we'll add the likelihoods and make the plots in the middle row of Figure 9.5.

```{r, fig.width = 3.5, fig.height = 3}
# D1: 3 heads, 12 tails
N <- 15
z <- 3

trial_data_1 <- rep(0:1, times = c(N - z, z))

# D2: 4 heads, 1 tail
N <- 5
z <- 4

trial_data_2 <- rep(0:1, times = c(N - z, z))
d <-
  d %>% 
  mutate(likelihood_1 = Bernoulli_likelihood(theta = theta_1, 
                                             data  = trial_data_1),
         likelihood_2 = Bernoulli_likelihood(theta = theta_2, 
                                             data  = trial_data_2))

# likelihood_1
d %>%
  ggplot(aes(x = theta_1, y = omega, fill = likelihood_1)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[1]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")

# likelihood_2
d %>%
  ggplot(aes(x = theta_2, y = omega, fill = likelihood_2)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[2]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

The likelihoods look good. Next I attempted to make the posteriors. Carefully compare the following plots with their analogues in Figure 9.5 and you'll see I've erred somewhere. If you maths are better than mine and you spot where I've gone wrong, [please share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

```{r, fig.width = 3.5, fig.height = 3}
d <-
  d %>% 
  mutate(posterior_1 = (omega * prior_1 * likelihood_1) / sum(omega * prior_1 * likelihood_1),
         posterior_2 = (omega * prior_2 * likelihood_2) / sum(omega * prior_2 * likelihood_2))

# posterior_1
d %>% 
  ggplot(aes(x = theta_1, y = omega, fill = posterior_1 * omega)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[1]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")

# posterior_2
d %>% 
  ggplot(aes(x = theta_2, y = omega, fill = posterior_2)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[2]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

See? The posterior for $\theta_2$ is concentrated to highly on the $\omega$-axis. You'll note that the next plot, the marginal for $\omega$, is a little too contentrated toward the top, too.

```{r, fig.width = 3.5, fig.height = 3}
d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior_2 + posterior_1)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("p(", omega, "|D)"))) +
  coord_flip() +
  theme(panel.grid = element_blank())
```

Here are the marginal plots on the bottom row of Figure 9.5.

```{r, fig.width = 3.5, fig.height = 3}
# for theta_1
d %>%
  group_by(theta_1) %>% 
  summarise(posterior = sum(posterior_1)) %>% 
  
  ggplot(aes(x = theta_1,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta[1]),
       y = expression(paste("p(", theta[1], "|D)"))) +
  coord_cartesian(ylim = c(0, .04)) +
  theme(panel.grid = element_blank())

# for theta_2
d %>%
  group_by(theta_2) %>% 
  summarise(posterior = sum(posterior_2)) %>% 
  
  ggplot(aes(x = theta_2,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta[2]),
       y = expression(paste("p(", theta[2], "|D)"))) +
  coord_cartesian(ylim = c(0, .04)) +
  theme(panel.grid = element_blank())
```

We'll do this dog and pony one more time for Figure 9.6, which uses different priors on the same data.

```{r, fig.width = 3.5, fig.height = 3}
alpha <- 2
beta  <- 2
kappa <- 75

d <-
  tibble(theta_1 = parameter_space,
         omega   = parameter_space) %>%
  expand(theta_1, omega) %>% 
  # the second column of theta values is a duplicate of the first
  mutate(theta_2 = theta_1) %>% 
  mutate(prior_1 = map2(theta_1, omega, make_prior),
         prior_2 = map2(theta_2, omega, make_prior)) %>% 
  unnest() %>% 
  # here we normalize
  mutate(prior_1 = prior_1 / sum(prior_1),
         prior_2 = prior_2 / sum(prior_2))

# 2-D prior density for theta_1
d %>% 
  ggplot(aes(x = theta_1, y = omega, fill = prior_1)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[1]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")

# marginal for omega
d %>%
  group_by(omega) %>% 
  summarise(prior = sum(prior_1 + prior_2)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("p(", omega, ")"))) +
  coord_flip(ylim = c(0, .08)) +
  theme(panel.grid = element_blank())

# marginal for theta_1
d %>%
  group_by(theta_1) %>% 
  summarise(prior = sum(prior_1 + prior_2)) %>% 
  
  ggplot(aes(x = theta_1,
             ymin = 0,
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta[1]),
       y = expression(paste("p(", theta[1], ")"))) +
  coord_cartesian(ylim = c(0, .08)) +
  theme(panel.grid = element_blank())
```

Likelihoods:

```{r, fig.width = 3.5, fig.height = 3}
d <-
  d %>% 
  mutate(likelihood_1 = Bernoulli_likelihood(theta = theta_1, 
                                             data  = trial_data_1),
         likelihood_2 = Bernoulli_likelihood(theta = theta_2, 
                                             data  = trial_data_2))

# likelihood_1
d %>%
  ggplot(aes(x = theta_1, y = omega, fill = likelihood_1)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[1]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")

# likelihood_2
d %>%
  ggplot(aes(x = theta_2, y = omega, fill = likelihood_2)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[2]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

The posteriors already look suspicious.

```{r, fig.width = 3.5, fig.height = 3}
d <-
  d %>% 
  mutate(posterior_1 = (likelihood_1 * prior_1 * omega) / sum(likelihood_1 * prior_1 * omega),
         posterior_2 = (likelihood_2 * prior_2 * omega) / sum(likelihood_2 * prior_2 * omega))

# posterior_1
d %>% 
  ggplot(aes(x = theta_1, y = omega, fill = posterior_1)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[1]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")

# posterior_2
d %>% 
  ggplot(aes(x = theta_2, y = omega, fill = posterior_2)) +
  geom_tile() +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta[2]),
       y = expression(omega)) +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

My attempt at the marginal for $\omega$.

```{r, fig.width = 3.5, fig.height = 3}
d %>%
  group_by(omega) %>% 
  summarise(posterior = sum(posterior_2 + posterior_1)) %>% 
  
  ggplot(aes(x = omega,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(omega),
       y = expression(paste("p(", omega, "|D)"))) +
  coord_flip(ylim = c(0, .04)) +
  theme(panel.grid = element_blank())
```

Yep, now it's clear something definately went wrong. Finally, here are the marginal plots on the bottom row of Figure 9.5.

```{r, fig.width = 3.5, fig.height = 3}
# for theta_1
d %>%
  group_by(theta_1) %>% 
  summarise(posterior = sum(posterior_1)) %>% 
  
  ggplot(aes(x = theta_1,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta[1], "|D)"))) +
  coord_cartesian(ylim = c(0, .04)) +
  theme(panel.grid = element_blank())

# for theta_2
d %>%
  group_by(theta_2) %>% 
  summarise(posterior = sum(posterior_2)) %>% 
  
  ggplot(aes(x = theta_2,
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta[2], "|D)"))) +
  coord_cartesian(ylim = c(0, .04)) +
  theme(panel.grid = element_blank())
```

Again, I've clearly erred somewhere. If you spot where I've gone wrong, [please share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

### A realistic model with MCMC.

Here' a look at the gamma densities of Figure 9.8.

```{r, fig.width = 6, fig.height = 5}
length <- 100

tibble(x = seq(from = 0, to = 200, length.out = length) %>% 
         rep(., times = 4)) %>% 
  mutate(shape = rep(c(.01, 1.56, 1, 6.25), each = length),
         rate = rep(c(.01, .0312, .02, .125), each = length)) %>% 
  mutate(mean = shape * 1/rate,
         sd = sqrt(shape * (1 / rate)^2)) %>% 
  mutate(label = str_c("shape = ", shape, ", rate = ", rate, 
                       "\nmean = ", mean, ", sd = ", sd)) %>% 
  
  ggplot(aes(x = x, 
             ymin = 0, 
             ymax = dgamma(x = x, shape = shape, rate = rate))) +
  geom_ribbon(fill = "grey67") +
  scale_y_continuous(breaks = c(0, .01, .02)) +
  coord_cartesian(xlim = 0:150) +
  labs(x = expression(kappa),
       y = expression(paste("p(", kappa, "|s, r)"))) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~label)
```

You can find the formulas for the mean and $SD$ for a given gamma distribution [here](http://astrostatistics.psu.edu/su07/R/html/stats/html/GammaDist.html). We use those formulas in the second `mutate()` statement for the data-prep stage of our Figure 9.8.

Kruschke's equations 9.7 and 9.8 are as follows:

$$s = \frac{\mu^2}{\sigma^2} \text{ and } r = \frac{\mu}{\sigma^2} \text{ for mean } \mu > 0$$

$$s = 1 + \omega r \text{ where } r = \frac{\omega + \sqrt{\omega^2 + 4\sigma^2}}{2\sigma^2} \text{ for mode } \omega > 0$$

With those in hand, we can follow Kruschke's "DBDA2E-utilities.R" file to make a couple convenience functions.

```{r}
gamma_s_and_r_from_mean_sd <- function(mean, sd) {
  if (mean <= 0) stop("mean must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  shape <- mean^2 / sd^2
  rate  <- mean   / sd^2
  return(list(shape = shape, rate = rate))
}

gamma_s_and_r_from_mode_sd <- function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate  <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
}
```

They're easy to put to use:

```{r}
gamma_s_and_r_from_mean_sd(mean = 10, sd = 100)

gamma_s_and_r_from_mode_sd(mode = 10, sd = 100)
```

```{r}
gamma_param <- gamma_s_and_r_from_mode_sd(mode = 10, sd = 100)

str(gamma_param)
```

### Doing it with ~~JAGS~~ brms.

Unlike JAGS, the brms `formula` will not correspond as closely to Figure 9.7. You'll see in just a bit.

### Example: Therapeutic touch.

```{r, warning = F, message = F}
my_data <- read_csv("data.R/TherapeuticTouchData.csv")

glimpse(my_data)
```

Here are what the data look like:

```{r, fig.width = 10, fig.height = 4}
my_data %>% 
  mutate(y = y %>% as.character()) %>% 
  
  ggplot(aes(x = y)) +
  geom_bar(aes(fill = stat(count))) +
  scale_y_continuous(breaks = seq(from = 0, to = 9, by = 3)) +
  scale_fill_viridis_c(option = "A", end = .7) +
  coord_flip() +
  theme(legend.position = "none",
        panel.grid = element_blank()) +
  facet_wrap(~s, ncol = 7)
```

And here's our Figure 9.9.

```{r, fig.width = 4, fig.height = 3.25}
my_data %>% 
  group_by(s) %>% 
  summarize(mean = mean(y)) %>%
  
  ggplot(aes(x = mean)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, binwidth = .1) +
  coord_cartesian(xlim = 0:1) +
  labs(x = "Proportion Correct",
       y = "# Practitioners") +
  theme(panel.grid = element_blank())
```

Let's open brms.

```{r, warning = F, message = F}
library(brms)
```

In applied statistics, the typical way to model a Bernoulli variable is with logistic regression. Instead of going through the pain of setting up a model in brms that mirror’s the one in the text, I’m going to set up a hierarchical logistic regression model, instead. 

Note the `family = bernoulli(link = "logit")` argument. In work-a-day regression with vanilla Gaussian variables, the prediction space is unbounded. But when we want to model the probability of a success for a Bernoulli variable (i.e., $\theta$), we need to constrain the model to only produce predictions between 0 and 1. With logistic regression, we use a link function to do just that. The consequence is that instead of modeling the probability, $\theta$, we’re modeling the logit probability.

In case you're curious, the logit of $\theta$ is:

$$logit(\theta) = log(\theta/(1 - \theta))$$

But anyway, we'll be doing logistic regression using the logit link. Kruschke will cover this in detail in chapter 21.

The next new part of our syntax is `(1 | s)`. As in the popular frequentist [lme4 package](https://cran.r-project.org/web/packages/lme4/index.html), you specify random effects or group-level parameters with the `(|)` syntax in brms. On the left side of the `|`, you tell brms what parameters you'd like to make random (i.e., vary by group). On the right side of the `|`, you tell brms what variable you want to group the parameters by. In our case, we want the intercepts to be grouped by `s`.

```{r fit1, cache = T, warning = F, message = F, results = 'hide'}
fit1 <-
  brm(data = my_data,
      family = bernoulli(link = "logit"),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4)
```

As it turns out, the $N(0, 1.5)$ prior is flat in the probability space for the intercept in a logistic regression model. We’ll explore that a little further down. The $N(0, 1)$ prior for the random effect is actually a half-Normal. That’s because brms defaults to bound $SD$ parameters to zero and above. The half $N(0, 1)$ prior for a hierarchical $SD$ parameter is weakly regularizing and is conservative in the sense that it presumes some pooling is preferable to no pooling. If you wanted to take a lighter approach, you might use something like a `cauchy(0, 5)`, instead. See the [prior wiki](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) by the Stan team for more. 

Here are the trace plots and posterior densities of the main parameters.

```{r, fig.width = 8, fig.height = 2.5}
plot(fit1)
```

The trace plots indicate no problems with convergence. We'll need to extract the posterior samples and open the bayesplot package before we can examine the autocorrelations.

```{r, warning = F, message = F}
post <- posterior_samples(fit1, add_chain = T)

library(bayesplot)
```

One of the nice things about bayesplot is it returns ggplot2 objects. As such, we can amend their theme settings to be consistent with our other ggplot2 plots. The `theme_set()` function will make that particularly easy. And since I prefer to plot without gridlines, we'll slip in a line on `panel.grid` to suppress them by default for the remainder of this chapter.

```{r}
theme_set(theme_grey() +
            theme(panel.grid = element_blank()))
```

Now we're ready for `bayesplot::mcmc_acf()`.

```{r, fig.width = 6, fig.height = 4, warning = F, message = F}
mcmc_acf(post, pars = c("b_Intercept", "sd_s__Intercept"), lags = 10)
```

It appears `fit1` had very low autocorrelations. Here we'll examine the $N_{eff}/N$ ratio.

```{r, fig.width = 6, fig.height = 3.5}
neff_ratio(fit1) %>% 
  mcmc_neff()
```

The $N_{eff}/N$ ratio values for our model parameters were excellent. And if you really wanted them, you could get the parameter labels on the y-axis by tacking `+ yaxis_text()` on at the end of the plot block.

Here's a numeric summary of the model:

```{r}
print(fit1)
```

We'll need `brms::inv_logit_scaled()` to convert the model parameters to predict $\theta$ rather than $logit(\theta)$. After the conversions, we'll be ready to make the histograms in the lower portion of Figure 9.10.

```{r, fig.width = 8, fig.height = 4, warning = F, message = F}
library(tidybayes)

post_small <-
  post %>% 
  # converting the linter model to the probability space with `inv_logit_scaled()`
  mutate(`theta[1]`  = (b_Intercept + `r_s[S01,Intercept]`) %>% inv_logit_scaled(),
         `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) %>% inv_logit_scaled(),
         `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) %>% inv_logit_scaled()) %>% 
  # making the difference distributions
  mutate(`theta[1] - theta[14]`  = `theta[1]`  - `theta[14]`,
         `theta[1] - theta[28]`  = `theta[1]`  - `theta[28]`,
         `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %>% 
  select(starts_with("theta"))

post_small %>% 
  gather() %>% 
  # this line is unnecessary, but will help order the plots 
  mutate(key = factor(key, levels = c("theta[1]", "theta[14]", "theta[28]", 
                                      "theta[1] - theta[14]", "theta[1] - theta[28]", "theta[14] - theta[28]"))) %>% 

  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 30) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  facet_wrap(~key, scales = "free", ncol = 3)
```

If you wanted the specific values of the posterior modes and 95% HDIs, you could do:

```{r}
post_small %>% 
  gather() %>%
  group_by(key) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

And here are the Figure 9.10 scatter plots.

```{r, fig.width = 8, fig.height = 2.5, warning = F, message = F}
library(gridExtra)

p1 <-
  post_small %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[14]`)) +
  geom_abline(linetype = 1, color = "white") +
  geom_point(color = "grey50", size = 1/8, alpha = 1/8) +
  coord_cartesian(xlim = 0:1,
                  ylim = 0:1)

p2 <-
  post_small %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[28]`)) +
  geom_abline(linetype = 1, color = "white") +
  geom_point(color = "grey50", size = 1/8, alpha = 1/8) +
  coord_cartesian(xlim = 0:1,
                  ylim = 0:1)

p3 <-
  post_small %>% 
  ggplot(aes(x = `theta[14]`, y = `theta[28]`)) +
  geom_abline(linetype = 1, color = "white") +
  geom_point(color = "grey50", size = 1/8, alpha = 1/8) +
  coord_cartesian(xlim = 0:1,
                  ylim = 0:1)

grid.arrange(p1, p2, p3, ncol = 3)
```

This is posterior distribution for the population estimate for $\theta$, which roughly corresponds to the upper right histogram of $\omega$ in Figure 9.10.

```{r, fig.width = 3.25, fig.height = 2.5}
# this part makes it easier to set the break points in `scale_x_continuous()` 
labels <-
  post %>% 
  transmute(theta = b_Intercept %>% inv_logit_scaled()) %>%
  mode_hdi() %>% 
  select(theta:.upper) %>% 
  gather() %>% 
  mutate(label = value %>% round(3) %>% as.character) %>% 
  slice(1:3)
  
post %>% 
  mutate(theta = b_Intercept %>% inv_logit_scaled()) %>% 

  ggplot(aes(x = theta)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 30) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = .95) +
  scale_x_continuous(breaks = labels$value,
                     labels = labels$label) +  
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(theta))
```

I'm not aware there's a straight conversion to get $\sigma$ in a probability metric. As far as I can tell, you have to first use `coef()` to "extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects" (p. 39 of the [user's guide for brms](https://cran.r-project.org/web/packages/brms/brms.pdf) version 2.5.0). With the model coefficient draws in hand, you can index them by posterior iteration, group them by that index, compute the iteration-level $SD$s, and then plot the distribution of the $SD$s.

```{r, fig.width = 3.25, fig.height = 2.5}
# the tibble of the primary data
sigmas <-
  coef(fit1, summary = F)$s %>% 
  as_tibble() %>% 
  mutate(iter = 1:n()) %>% 
  group_by(iter) %>% 
  gather(key, value, -iter) %>% 
  mutate(theta = inv_logit_scaled(value)) %>% 
  summarise(sd = sd(theta))

# this, again, is just to customize `scale_x_continuous()`
labels <-
  sigmas %>% 
  mode_hdi(sd) %>% 
  select(sd:.upper) %>% 
  gather() %>% 
  mutate(label = value %>% round(3) %>% as.character) %>% 
  slice(1:3)
  
# the plot
sigmas %>% 
  ggplot(aes(x = sd)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 30, boundary = 0) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = .95) +
  scale_x_continuous(breaks = labels$value,
                     labels = labels$label) +  
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(paste(sigma, " of ", theta, " in a probability metric")))
```

And now you have a sense of how to do all those by hand, `bayesplot::mcmc_pairs()` offers a farily quick way to get a good portion of Figure 9.10. 

```{r, fig.width = 5.5, fig.height = 5, warning = F, message = F}
color_scheme_set("gray")
 
coef(fit1, summary = F)$s %>% 
  inv_logit_scaled() %>% 
  as_tibble() %>% 
  rename(`theta[1]` = S01.Intercept, 
         `theta[14]` = S14.Intercept, 
         `theta[28]` = S28.Intercept) %>% 
  select(`theta[1]`, `theta[14]`, `theta[28]`) %>% 
  
  mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8))
```

Did you see how we slipped in that `color_scheme_set("gray")` line? When we used `theme_set()`, earlier, that changed the global theme settings for our ggplot2 plots. The `color_scheme_set()` function is specific to bayesplot plots and it sets the color palette within them. Setting the color palette "gray" changed the colors depicted in the dots and bars of the `mcmc_pairs()`-based scatter plots and histograms, respectively. 

Kruschke used a beta(1, 1) prior for $\omega$. If you randomly draw from that prior and plot a histogram, you’ll see it was flat.

```{r, fig.width = 4, fig.height = 3}
set.seed(1)
tibble(prior = rbeta(n = 1e5, 1, 1)) %>% 
  ggplot(aes(x = prior)) +
  geom_histogram(color = "grey92", fill = "grey67", size = .2,
                 binwidth = .05, boundary = 0) +
  scale_x_continuous(labels = c("0", ".25", ".5", ".75", "1")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:1) +
  xlab(expression(omega)) +
  theme(legend.position = "none")
```

You'll note that plot corresponds to the upper right panel of Figure 9.11. 

Recall that we used a logistic regression model with a `normal(0, 1.5)` prior on the intercept. If you sample from `normal(0, 1.5)` and then convert the draws using `brms::inv_logit_scaled()`, you'll discover that our `normal(0, 1.5)` prior was virtually flat on the probability scale. Here we'll show the consequence of a variety of zero-mean Gaussian priors for the intercept of a logistic regression model:

```{r prior_sim, cache = T, fig.width = 8, fig.height = 6}
r_norm <- function(i, n = 1e4){
  set.seed(1)
  rnorm(n = n, mean = 0, sd = i) %>% 
    inv_logit_scaled()
}

tibble(sd = seq(from = .25, to = 3, by = .25)) %>% 
  group_by(sd) %>% 
  mutate(prior = map(sd, r_norm)) %>% 
  unnest() %>% 
  ungroup() %>% 
  mutate(sd = str_c("sd = ", sd)) %>% 
  
  ggplot(aes(x = prior)) +
  geom_histogram(fill = "grey67", color = "grey92", size = .2,
                 binwidth = .05, boundary = 0) +
  scale_x_continuous(labels = c("0", ".25", ".5", ".75", "1")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:1) +
  facet_wrap(~sd)
```

It appears that as $\sigma$ goes lower than 1.25, the prior becomes increasingly regularizing, pulling the estimate for $\theta$ to a neutral .5. However, as the prior’s $\sigma$ gets larger than 1.25, more and more of the probability mass ends up at extreme values.

Here's the diagonal of the lower grid of Figure 9.11.

```{r, fig.width = 8, fig.height = 2.5}
n <- 1e4

set.seed(1)
intercept_prior <- rnorm(n = n, mean = 0, sd = 1.5)
sigma_prior     <- rnorm(n = n, mean = 0, sd = 1) %>% abs()
 
prior_samples <-
  tibble(`theta[1]`  = rnorm(n = n, mean = intercept_prior, sd = sigma_prior) %>% inv_logit_scaled(),
         `theta[14]` = rnorm(n = n, mean = intercept_prior, sd = sigma_prior) %>% inv_logit_scaled(),
         `theta[28]` = rnorm(n = n, mean = intercept_prior, sd = sigma_prior) %>% inv_logit_scaled())

prior_samples %>% 
  gather() %>% 

  ggplot(aes(x = value)) +
  geom_histogram(fill = "grey67", color = "grey92", size = .2,
                binwidth = .05, boundary = 0) +
  scale_x_continuous(labels = c("0", ".25", ".5", ".75", "1")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:1) +
  facet_wrap(~key)
```

With a little subtraction, we can reproduce the plots in the upper triangle.

```{r, fig.width = 8, fig.height = 2.5}
prior_samples %>% 
  mutate(`theta[1] - theta[14]`  = `theta[1]`  - `theta[14]`,
         `theta[1] - theta[28]`  = `theta[1]`  - `theta[28]`,
         `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %>% 
  select(contains("] - t")) %>% 
  gather() %>% 

  ggplot(aes(x = value)) +
  geom_histogram(fill = "grey67", color = "grey92", size = .2,
                binwidth = .05, boundary = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  facet_wrap(~key)
```

Those plots clarify our hierarchical logistic regression model was a little more regularizing than Kruschke's. The consequence of our priors was more aggressive regularization, greater shrinkage toward zero. The prose in the next section of the text clarifies this isn’t necessarily a bad thing.

Finally, here are the plots for the lower triangle in Figure 9.11.

```{r, fig.width = 8, fig.height = 2.5}
p1 <-
  prior_samples %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[14]`)) +
  geom_point(color = "grey50", size = 1/8, alpha = 1/8) +
  geom_abline(linetype = 1, color = "white") +
  coord_cartesian(xlim = 0:1,
                  ylim = 0:1)

p2 <-
  prior_samples %>% 
  ggplot(aes(x = `theta[1]`, y = `theta[28]`)) +
  geom_point(color = "grey50", size = 1/8, alpha = 1/8) +
  geom_abline(linetype = 1, color = "white") +
  coord_cartesian(xlim = 0:1,
                  ylim = 0:1)

p3 <-
  prior_samples %>% 
  ggplot(aes(x = `theta[14]`, y = `theta[28]`)) +
  geom_point(color = "grey50", size = 1/8, alpha = 1/8) +
  geom_abline(linetype = 1, color = "white") +
  coord_cartesian(xlim = 0:1,
                  ylim = 0:1)

grid.arrange(p1, p2, p3, ncol = 3)
```

## Shrinkage in hierarchical models

Recall formula 9.4 from page 223.

$$\theta \sim \text{dbeta}(\omega(\kappa - 2) + 1), (1 - \omega)(\kappa - 2) + 1)$$

With that formula, we can express `dbeta()`'s `shape1` and `shape2` in terms of $\omega$ and $\kappa$ and make the shapes in Figure 9.12.

```{r, fig.width = 6, fig.height = 2.5}
omega  <- 0.5
kappa1 <- 2.1
kappa2 <- 15.8

tibble(x = seq(from = 0, to = 1, by = .001)) %>%
  mutate(`kappa = 2.1` = dbeta(x = x, 
                               shape1 = omega * (kappa1 - 2) + 1, 
                               shape2 = (1 - omega) * (kappa1 - 2) + 1),
         `kappa = 15.8` = dbeta(x = x, 
                                shape1 = omega * (kappa2 - 2) + 1, 
                                shape2 = (1 - omega) * (kappa2 - 2) + 1)) %>% 
  gather(key, value, - x) %>% 
  mutate(key = factor(key, levels = c("kappa = 2.1", "kappa = 15.8"))) %>% 
  
  ggplot(aes(x = x, 
             ymin = 0,
             ymax = value)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(paste("Data Proportion or ", theta, " value")),
       y = expression(paste("dbeta(", theta, "|", omega, ",", kappa, ")"))) +
  facet_wrap(~key)
```

## Speeding up ~~JAGS~~ brms

Here we'll compare the time it takes to fit `fit1` as either `bernoulli(link = "logit")` or `binomial(link = "logit")`.

```{r bernoulli_vs_binomial, cache = T, message = F, warning = F, results = 'hide'}
# bernoulli
start_time_bernoulli <- proc.time()
brm(data = my_data,
      family = bernoulli(link = "logit"),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4)
stop_time_bernoulli <- proc.time()

# binomial
start_time_binomial <- proc.time()
brm(data = my_data,
      family = bernoulli(link = "logit"),
      y ~ 1 + (1 | s),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4)
stop_time_binomial <- proc.time()
```

See how we're using `proc.time()` to record when we began and finished evaluating our `brm()` code? The last time we covered that was way back in chapter 3. In chapter 3 we also learned how subtracting the former from the latter yields the total elapsed time.

```{r}
stop_time_bernoulli - start_time_bernoulli
stop_time_binomial - start_time_binomial
```

If you wanted to be rigorous about this, you could do this multiple times in a mini simulation.

As to the issue of parallel processing, we’ve been doing this all along. Note our `chains = 4, cores = 4` code.

## Extending the hierarchy: Subjects within categories

### Example: Baseball batting abilities by position.

Here are the batting average data, `ba`.

```{r, warning = F, message = F}
ba <- read_csv("data.R/BattingAverage.csv")

glimpse(ba)
```

And here are the number of occasions by primary position, `PriPos`, with their median at bat, `AtBats`, values.

```{r}
ba %>% 
  group_by(PriPos) %>% 
  summarise(n      = n(),
            median = median(AtBats)) %>% 
  arrange(desc(n))
```

As these data are aggregated, we'll fit with an aggregated binomial model. This is still logistic regression. The Bernoulli distribution is a special case of the binomial distribution when the number of trials in each data point is 1 (see [this vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html) for details). Since our data are aggregated, the information encoded in `Hits` is a combination of multiple trials, which requires us to jump up to the more general binomial likelihood. Note the `Hits  | trials(AtBats)` syntax. With that bit, we instructed brms that our criterion, `Hits`, is an aggregate of multiple trials and the number of trials is encoded in `AtBats`. 

Also note the `(1 | PriPos) + (1 | PriPos:Player)` syntax. In this model, we have two grouping factors, `PriPos` and `Player`. Thus we have two `(|)` arguments. But since players are themselves nested within positions, we have encoded that nesting with the `(1 | PriPos:Player)` syntax. For more on this style of syntax, see [Kristoffer Magnusson’s handy post](http://rpsychologist.com/r-guide-longitudinal-lme-lmer#different-level-3-variance-covariance-matrix). Since brms syntax is based on that from the earlier nlme and lme4 packages, the basic syntax rules apply. Bürkner, of course, also covers these topics in the `brmsformula` subsection of his [brms reference manual](https://cran.r-project.org/web/packages/brms/brms.pdf).

```{r fit2, cache = T, warning = F, message = F, results = 'hide'}
fit2 <-
  brm(data = ba,
      family = binomial(link = "logit"),
      Hits  | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player),
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 1), class = sd)),
      iter = 3500, warmup = 500, chains = 3, cores = 3,
      control = list(adapt_delta = .99))
```

The chains look good.

```{r, fig.width = 10, fig.height = 4}
color_scheme_set("blue")

plot(fit2)
```

We might examine the autocorrelations within the chains.

```{r, fig.width = 8, fig.height = 4, warning = F, message = F}
post <- posterior_samples(fit2, add_chain = T)

mcmc_acf(post, pars = c("b_Intercept", 
                        "sd_PriPos__Intercept", 
                        "sd_PriPos:Player__Intercept"), lags = 8)
```

Here's a histogram of the $N_{eff}/N$ ratios.

```{r, fig.width = 6, fig.height = 3}
fit2 %>% 
  neff_ratio() %>% 
  mcmc_neff_hist(binwidth = .1) +
  yaxis_text()
```

Happily, most have a very favorable ratio. Here's a numeric summary of the primary model parameters.

```{r}
print(fit2)
```

As far as I’m aware, brms offers three major ways to get the group-level parameters for a hierarchical model: using `posterior_samples()`, `coef()`, or `fitted()`. I’ll demonstrate each, in turn. We'll begin with `posterior_samples()`. In order to look at the autocorrelation plots, above, we already saved the `posterior_samples(fit2)` output as `post`. Here we'll look at its structure with `head()`. Before doing so we'll convert `post`, which is currently saved as a data frame, into a tibble in order to keep the output from getting unwieldy.

```{r}
post <-
  post %>% 
  as_tibble()

head(post)
```

In the text, Kruschke described the model as having 968 parameters. Our `post` tibble has one vector for each, with a couple others tacked onto the end. In the hierarchical logistic regression model, the group-specific parameters for the levels of `PriPos` are additive combinations of the global intercept vector, `b_Intercept` and each position-specific vector, `r_PriPos[i.Base,Intercept]`, where `i` is a fill-in for the position of interest. And recall that since the linear model is of the logit of the criterion, we’ll need to use `inv_logit_scaled()` to convert that to the probability space.

```{r}
post_small <-
  post %>% 
  transmute(`1st Base` = (b_Intercept + `r_PriPos[1st.Base,Intercept]`), 
            Catcher    = (b_Intercept + `r_PriPos[Catcher,Intercept]`), 
            Pitcher    = (b_Intercept + `r_PriPos[Pitcher,Intercept]`)) %>% 
  mutate_all(inv_logit_scaled) %>% 
  # here we compute our difference distributions
  mutate(`Pitcher - Catcher`  = Pitcher - Catcher,
         `Catcher - 1st Base` = Catcher - `1st Base`)

head(post_small)
```

If you take a glance at Figures 9.14 through 9.16, we'll be making a lot of histograms of the same basic structure. We can make a histogram plotting function to streamline our code.

```{r}
make_histogram <- function(data, mapping, title, xlim, ...){
  ggplot(data, mapping) +
  geom_histogram(fill = "grey67", color = "grey92", size = .2,
                bins = 30) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = title,
       x     = expression(theta)) +
  coord_cartesian(xlim = xlim) +
  theme(legend.position = "none")
}
```

We'll do the same thing for the correlation plots.

```{r}
make_point <- function(data, mapping, limits, ...){
  ggplot(data, mapping) +
  geom_abline(color = "white") +
  geom_point(color = "grey50", size = 1/10, alpha = 1/20) +
  coord_cartesian(xlim = limits,
                  ylim = limits)
}
```

To learn more about wrapping custom plots into custom functions, check out chapter 12 of Wickham's [*ggplot2, Elegant graphics for data analysis*](https://github.com/hadley/ggplot2-book).

Now we have our `make_histogram()` and `make_point()` functions, we'll use `grid.arrange()` to paste together the left half of Figure 9.14.

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = post_small,
                 aes(x = Pitcher), 
                 title = "Pitcher", 
                 xlim = c(.1, .25))

p2 <-
  make_histogram(data = post_small,
                 aes(x = `Pitcher - Catcher`), 
                 title = "Pitcher - Catcher", 
                 xlim = c(-.15, 0))

p3 <-
  make_point(data = post_small,
             aes(x = Pitcher, y = Catcher),
             limits = c(.12, .25))

p4 <-
  make_histogram(data = post_small,
                 aes(x = Catcher), 
                 title = "Catcher", 
                 xlim = c(.1, .25))

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

We could follow the same procedure to make the right portion of Figure 9.14. But instead, let’s switch gears and explore the second way brms affords us for plotting group-level parameters. This time, we’ll use `coef()`.

Up in section 9.2.4, we learned that we can use `coef()` to "extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects" (p. 39 of the [user's guide for brms](https://cran.r-project.org/web/packages/brms/brms.pdf) version 2.5.0). The grouping level we’re interested in is ` PriPos`, so we’ll use that to index the information returned by `coef()`. Since `coef()` returns a matrix, we'll use `as_tibble()` to convert it to a tibble.

```{r}
coef_primary_position <-
  coef(fit2, summary = F)$PriPos %>% 
  as_tibble()
  
str(coef_primary_position)
```

Keep in mind that `coef()` returns the values in the logit scale when used for logistic regression models. So we’ll have to use `brms::inv_logit_scaled()` to convert the estimates to the probability metric. After we’re done converting the estimates, we’ll then make the difference distributions.

```{r}
coef_small <-
  coef_primary_position %>% 
  select(`1st Base.Intercept`, Catcher.Intercept, Pitcher.Intercept) %>% 
  transmute(`1st Base` = `1st Base.Intercept`, 
            Catcher    = Catcher.Intercept, 
            Pitcher    = Pitcher.Intercept) %>% 
  mutate_all(inv_logit_scaled) %>% 
  # here we make the difference distributions
  mutate(`Pitcher - Catcher`  = Pitcher - Catcher,
         `Catcher - 1st Base` = Catcher - `1st Base`)

head(coef_small)
```

Now we're ready for the right half of Figure 9.14.

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = coef_small,
                 aes(x = Catcher), 
                 title = "Catcher", 
                 xlim = c(.22, .27))

p2 <-
  make_histogram(data = coef_small,
                 aes(x = `Catcher - 1st Base`), 
                 title = "Catcher - 1st Base", 
                 xlim = c(-.04, .01))

p3 <-
  make_point(data = coef_small,
             aes(x = Catcher, y = `1st Base`),
             limits = c(.22, .27))

p4 <-
  make_histogram(data = coef_small,
                 aes(x = `1st Base`), 
                 title = "1st Base", 
                 xlim = c(.22, .27))

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

And if you wanted the posterior modes and HDIs, you'd use `mode_hdi()` after a little wrangling.

```{r}
coef_small %>% 
  gather() %>% 
  group_by(key) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

While we're at it, we should capitalize on the opportunity to show how these results are the same as those derived from our `posterior_samples()` approach, above.

```{r}
post_small %>% 
  gather() %>% 
  group_by(key) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

For Figures 9.15 and 9.16, Kruschke drilled down further into the posterior. To drill along with him, we'll take the opportunity to showcase `fitted()`, the third way brms affords us for plotting group-level parameters.

```{r, fig.width = 3.5, fig.height = 2.5}
# this will make life easier. just go with it
name_list <- c("Kyle Blanks", "Bruce Chen", "ShinSoo Choo", "Ichiro Suzuki", 
               "Mike Leake", "Wandy Rodriguez", "Andrew McCutchen", "Brett Jackson")

# we'll define the data we'd like to feed into `fitted()`, here
nd <-
  ba %>% 
  filter(Player %in% c(name_list)) %>% 
  # these last two lines aren't typically necessary, 
  # but they allow us to arrange the rows in the same order we find the names in Figures 9.15 and 9.16
  mutate(Player = factor(Player, levels = c(name_list))) %>% 
  arrange(Player)

fitted_players <-
  fitted(fit2, 
       newdata = nd,
       scale = "linear",
       summary = F) %>% 
  as_tibble() %>% 
  # rename the values as returned by `as_tibble()`
  transmute(`Kyle Blanks`      = V1, 
            `Bruce Chen`       = V2, 
            `ShinSoo Choo`     = V3,
            `Ichiro Suzuki`    = V4,
            `Mike Leake`       = V5,
            `Wandy Rodriguez`  = V6,
            `Andrew McCutchen` = V7,
            `Brett Jackson`    = V8) %>% 
  # convert the values from the logit scale to the probability scale
  mutate_all(inv_logit_scaled) %>% 
  # in this last section, we make our difference distributions 
  mutate(`Kyle Blanks - Bruce Chen`         = `Kyle Blanks`      - `Bruce Chen`,
         `ShinSoo Choo - Ichiro Suzuki`     = `ShinSoo Choo`     - `Ichiro Suzuki`,
         `Mike Leake - Wandy Rodriguez`     = `Mike Leake`       - `Wandy Rodriguez`,
         `Andrew McCutchen - Brett Jackson` = `Andrew McCutchen` - `Brett Jackson`)
    
glimpse(fitted_players)
```

Note our use of the `scale = "linear"` argument in the `fitted()` function. By default, `fitted()` returns predictions on the scale of the criterion. But we don't want a list of successes and failures; we want player-level parameters. When you specify `scale = "linear"`, you request `fitted()` return the values in the parameter scale.

Figure 9.15, left:

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `Kyle Blanks`), 
                 title = "Kyle Blanks (1st Base)", 
                 xlim = c(.05, .35))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `Kyle Blanks - Bruce Chen`), 
                 title = "Kyle Blanks (1st Base) -\nBruce Chen (Pitcher)", 
                 xlim = c(-.1, .25))

p3 <-
  make_point(data = fitted_players,
             aes(x = `Kyle Blanks`, y = `Bruce Chen`),
             limits = c(.09, .35))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Bruce Chen`), 
                 title = "Bruce Chen (Pitcher)", 
                 xlim = c(.05, .35))

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

Figure 9.15, right:

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `ShinSoo Choo`), 
                 title = "ShinSoo Choo (Right Field)", 
                 xlim = c(.22, .34))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `ShinSoo Choo - Ichiro Suzuki`), 
                 title = "ShinSoo Choo (Right Field) -\nIchiro Suzuki (Right Field)", 
                 xlim = c(-.07, .07))

p3 <-
  make_point(data = fitted_players,
             aes(x = `ShinSoo Choo`, y = `Ichiro Suzuki`),
             limits = c(.23, .32))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Ichiro Suzuki`), 
                 title = "Ichiro Suzuki (Right Field)", 
                 xlim = c(.22, .34))

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

Figure 9.16, left:

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `Mike Leake`), 
                 title = "Mike Leake (Pitcher)", 
                 xlim = c(.05, .35))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `Mike Leake - Wandy Rodriguez`), 
                 title = "Mike Leake (Pitcher) -\nWandy Rodriguez (Pitcher)", 
                 xlim = c(-.05, .25))

p3 <-
  make_point(data = fitted_players,
             aes(x = `Mike Leake`, y = `Wandy Rodriguez`),
             limits = c(.07, .25))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Wandy Rodriguez`), 
                 title = "Wandy Rodriguez (Pitcher)", 
                 xlim = c(.05, .35))

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

Figure 9.16, right:

```{r, fig.width = 6, fig.height = 4.5}
p1 <-
  make_histogram(data = fitted_players,
                 aes(x = `Andrew McCutchen`), 
                 title = "Andrew McCutchen (Center Field)", 
                 xlim = c(.15, .35))

p2 <-
  make_histogram(data = fitted_players,
                 aes(x = `Andrew McCutchen - Brett Jackson`), 
                 title = "Andrew McCutchen (Center Field) -\nBrett Jackson (Center Field)", 
                 xlim = c(0, .20))

p3 <-
  make_point(data = fitted_players,
             aes(x = `Andrew McCutchen`, y = `Brett Jackson`),
             limits = c(.15, .35))

p4 <-
  make_histogram(data = fitted_players,
                 aes(x = `Brett Jackson`), 
                 title = "Brett Jackson (Center Field)", 
                 xlim = c(.15, .35))

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

And if you wanted the posterior modes and HDIs, you'd use `mode_hdi()` after a little wrangling.

```{r}
fitted_players %>% 
  gather() %>% 
  group_by(key) %>% 
  mode_hdi(value) %>% 
  mutate_if(is.double, round, digits = 3)
```

## References {-}

Kruschke, J. K. (2015). *Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan.* Burlington, MA: Academic Press/Elsevier.

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, warning = F, echo = F}
# Here we'll remove our objects
rm(beta_by_k, alpha, beta, make_prior, parameter_space, kappa, d, text, Bernoulli_likelihood, N, z, trial_data, length, gamma_s_and_r_from_mean_sd, gamma_s_and_r_from_mode_sd, gamma_param, my_data, fit1, post, post_small, p1, p2, p3, labels, sigmas, r_norm, n, prior_samples, omega, kappa1, kappa2, stop_time_bernoulli, start_time_bernoulli, stop_time_binomial, start_time_binomial, ba, fit2, make_histogram, make_point, p4, coef_primary_position, coef_small, name_list, nd, fitted_players)

theme_set(theme_grey())
```

